{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#DQN&DDQN_RIGHTONLY.ipynb trains the agent 5000 times in right only actions with DQN and DDQN algorithms."
      ],
      "metadata": {
        "id": "5MvRCY1meOMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F53Es-JWmv63",
        "outputId": "ed67c639-30a4-4784-a2ec-1baf42b1836e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC6GDXjdnMz9",
        "outputId": "e1426b58-0020-443f-f338-453586c43d27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym-super-mario-bros==7.4.0\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "Collecting nes-py>=8.1.4\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.21.6)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.64.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.10.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py): started\n",
            "  Building wheel for nes-py (setup.py): finished with status 'done'\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp38-cp38-linux_x86_64.whl size=439734 sha256=dafdd6e9286b259838418386c751cf687b7e427dfdab5097069e24f6afb046e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/e5/5c/8dfae61b44dbf56c458483aa09accef55a650e0527f6cbd872\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install gym-super-mario-bros==7.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab48E-rZoI_t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "import gym\n",
        "import numpy as np\n",
        "import collections\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "class MSE(gym.Wrapper):\n",
        "    \"\"\"\n",
        "        Adjust the game screen output of the Mario game, because when playing the game, \n",
        "        the continuity of the game will make most of the continuous pictures obtained \n",
        "        within a period of time approximate. In order to reduce the input of approximate \n",
        "        game scene pictures and improve training efficiency, it will Traverse the skip \n",
        "        frame image, calculate the sum of the skip frame image rewards, and perform a \n",
        "        maximum pooling in the last 2 frames of the skip frame, and generate a frame \n",
        "        image after pooling to represent the skip frame image. At here, skip=4.\n",
        "    \"\"\"\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MSE, self).__init__(env)\n",
        "        #for the most recent raw observation, aka for max pooling across time steps\n",
        "        self.buffer_observation = collections.deque(maxlen=2)\n",
        "        self.skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        reward_total = 0.0\n",
        "        for _ in range(self.skip):\n",
        "            observation, reward, done, info = self.env.step(action)\n",
        "            self.buffer_observation.append(observation)\n",
        "            reward_total = reward_total + reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self.buffer_observation), axis=0)\n",
        "        return max_frame, reward_total, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Skip frames, and max pooling is implemented here\"\"\"\n",
        "        self.buffer_observation.clear()\n",
        "        observation = self.env.reset()\n",
        "        self.buffer_observation.append(observation)\n",
        "        return observation\n",
        "\n",
        "class MR84x84(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Set the size of the game screen Adjust the size of the game screen \n",
        "    to 84*84 grayscale image, ie shape 84,84,1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env=None):\n",
        "        super(MR84x84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return MR84x84.process(obs)\n",
        "\n",
        "    def process(frame):\n",
        "        if frame.size == 240 * 256 * 3:\n",
        "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "            # image normalization\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "class imgToTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Convert 84*84*1 array to 1*84*84 tensor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(imgToTorch, self).__init__(env)\n",
        "        original_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(original_shape[-1], original_shape[0], original_shape[1]),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "class BW(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Function: return the picture of consecutive n_steps frames (after skip)\n",
        "    Implementation: When env calls the reset() method, self.buffer will be initialized, \n",
        "    and its size will be initialized to n_steps*84*84, that is, four 1*84*84 game screens \n",
        "    can be placed. When initializing, the first three It is all 0, the last one is the initialized game screen\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BW, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        # self.env.reset() is the screen where the game starts as a parameter of self.observation\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # Move the last three positions forward, the first position will be covered\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        # Put observation to the last position\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "class PixelNormalization(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Normalize 4*84*84 data so that the pixel values ​​are distributed between 0-1, a common \n",
        "    operation of convolutional neural networks\n",
        "    \"\"\"\n",
        "\n",
        "    def observation(self, obs):\n",
        "        normalization = np.array(obs).astype(np.float32) / 255.0\n",
        "        return normalization\n",
        "\n",
        "'''\n",
        "The above classes are all inherited from the classes in gym\n",
        "'''\n",
        "\n",
        "class DQN_network(nn.Module):\n",
        "    \"\"\"\n",
        "        The most primitive Q-learning algorithm always needs a Q table to record during the execution \n",
        "        process. When the dimension is not high, the Q table can still meet the demand, but when encountering \n",
        "        exponential dimension, the efficiency of the Q table is very high. limited. Therefore, we consider a \n",
        "        value function approximation method, so that each time we only need to know S or A in advance, we can \n",
        "        get the corresponding Q value in real time.\n",
        "\n",
        "        Originally, the Q value was found through S (state) and A (action), but now a neural network is used to \n",
        "        obtain an approximate Q value. The neural network is very powerful, so we can directly use the game screen\n",
        "        as a state S and input it into the network. We can use the output of the network as the Q value, and make \n",
        "        each output correspond to the Q value of an action A, so that we can Get the Q value of each action in state S. \n",
        "        This is DQN.\n",
        "\n",
        "        For this Mario game, if we only look at a certain frame alone, we must be missing some relevant information. \n",
        "        For example, we cannot judge whether Mario is rising or falling through one frame. So it is estimated that if \n",
        "        several consecutive frames are input at the same time, the neural network can know whether Mario is rising or \n",
        "        falling, and it can be estimated that this will be better as an input. This is why the first few classes that \n",
        "        inherit from gym have transformed Mario's output game screen.\n",
        "\n",
        "    This class is the construction of the convolutional neural network. The input of the convolutional neural network \n",
        "    is an 84*84*4 game scene, and the output is action_space, corresponding to the Q value of each action.    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN_network, self).__init__()\n",
        "        # Build convolutional layers\n",
        "        self.conv = nn.Sequential(\n",
        "            # input_shape[0] The number of channels of the input image, the superposition of 4 single-channel images is 4\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Get the output of the convolutional layer and expand it into 1 dimension, then input the size of the fully connected layer\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "\n",
        "        # Build a fully connected layer\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)\n",
        "\n",
        "class DQN_agent:\n",
        "    def __init__(self, state_space, action_space, max_memory_size, batch_size, gamma, lr,\n",
        "                 dropout, exploration_max, exploration_min, exploration_decay, double_dqn, pretrained,save_path):\n",
        "        '''\n",
        "            Initialization parameters:\n",
        "                1. state_space: game state space, that is, the shape (4,84,84) of the game state obtained after processing the env screen above represents four consecutive game scenes\n",
        "                2.action_space: game action space, that is, the number of actions RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT corresponds to 5 7 12\n",
        "                3.max_memory_size: The size of the experience playback pool\n",
        "                    The concept and purpose of experience playback: Reinforcement learning has stability problems due to the correlation between states, because the samples collected by \n",
        "                    the agent when exploring the environment are a time series, and there is continuity between the samples, so it is necessary to break the time correlation , the solution \n",
        "                    is to store the current training state to the memory M during training, and randomly sample mini-batches from M for updating when updating parameters.\n",
        "                    Specifically, the data type stored in M is <s,a,r,s′,done>:\n",
        "                        a: action performed by state s\n",
        "                        s: state s before the execution of state a\n",
        "                        s′: the new state s after state a is executed\n",
        "                        r: the reward obtained by executing a in state s\n",
        "                        done: whether the game is over\n",
        "                    M has a maximum length limit to ensure that the updated data are all recent data. The ultimate goal of experience replay is to prevent overfitting.\n",
        "                4. batch_size: the batch of each training, that is, each time the batch_size group data is randomly selected from the experience pool for training\n",
        "                5. Gamma: 0-1\n",
        "                    ddqn according to Q*(S, A) <- r + gamma * max_a Q_target(S', a)\n",
        "                    dqn according to Q*(S, A) <- r + gamma * max_a Q(S', a)\n",
        "                    It can be seen that the Q value is the sum of the current reward and the predicted Q value, and gamma indicates the value of the future\n",
        "                6.lr: learning rate\n",
        "                7. exploration_max: The probability of random exploration is the probability of taking random actions\n",
        "                    Purpose: The initial network has not been trained, directly using the network output will cause the same scene to be stuck directly, so\n",
        "                             Use random actions to obtain training data and explore the state space\n",
        "                8.exploration_min: The lowest probability of random exploration. With the training of the network, the network will become better and better, and the space for exploration will become more and more, so\n",
        "                    The probability of using random exploration needs to be slowly reduced until it falls to the preset minimum value\n",
        "                9.exploration_decay: random exploration decay factor, now_exploration=exploration*exploration_decay, is a value between 0-1, less than 1\n",
        "                10.double_dqn: the agent type is DDQN or DQN, the optimization goals of the two are different\n",
        "                11.pretrained: whether it is a test, if it is a test, just test, no training is required\n",
        "                12.save_path: test model loading path\n",
        "        '''\n",
        "        # Define DQN Layers\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.double_dqn = double_dqn\n",
        "        self.pretrained = pretrained\n",
        "        # If there is a GPU, use GPU training, otherwise CPU\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Double DQN network\n",
        "        if self.double_dqn:\n",
        "            # Use Net to create two neural networks: evaluation network and target network\n",
        "            self.local_net = DQN_network(state_space, action_space).to(self.device)\n",
        "            self.target_net = DQN_network(state_space, action_space).to(self.device)\n",
        "            # If it is a test, load the trained model from memory\n",
        "            if self.pretrained:\n",
        "                self.local_net.load_state_dict(torch.load(os.path.join(save_path,\"DQN1.pt\"), map_location=torch.device(self.device)))\n",
        "                self.target_net.load_state_dict(torch.load(os.path.join(save_path,\"DQN2.pt\"), map_location=torch.device(self.device)))\n",
        "            # Optimizer selects Adam\n",
        "            self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
        "            # #Copy local_net weights to target_net every 5000 steps\n",
        "            self.copy = 5000\n",
        "            self.step = 0\n",
        "        # DQN network\n",
        "        else:\n",
        "            # Only need to load a network\n",
        "            self.dqn = DQN_network(state_space, action_space).to(self.device)\n",
        "            if self.pretrained:\n",
        "                self.dqn.load_state_dict(torch.load(os.path.join(save_path,\"DQN.pt\"), map_location=torch.device(self.device)))\n",
        "            self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=lr)\n",
        "        # Create memory\n",
        "        self.max_memory_size = max_memory_size\n",
        "        #The entire experience pool is composed of STATE_MEMORY ACTION_MEMORY STATE2_MEMORY REWARD_MEMORY DONE_MEMORY, each of which is a torch tensor with a length of max_memory_size\n",
        "        #These five components (s, a, s', done)\n",
        "        if self.pretrained:\n",
        "            self.ACTION_MEMORY = torch.load(os.path.join(save_path,\"ACTION_MEMORY.pt\"))\n",
        "            self.REWARD_MEMORY = torch.load(os.path.join(save_path,\"REWARD_MEMORY.pt\"))\n",
        "            self.DONE_MEMORY = torch.load(os.path.join(save_path,\"DONE_MEMORY.pt\"))\n",
        "            with open(os.path.join(save_path,\"terminal_position.pkl\"), 'rb') as f:\n",
        "                self.terminal_position = pickle.load(f)\n",
        "            with open(os.path.join(save_path,\"num_in_queue.pkl\"), 'rb') as f:\n",
        "                self.num_in_queue = pickle.load(f)\n",
        "        else:\n",
        "            self.ACTION_MEMORY = torch.zeros(max_memory_size, 1)\n",
        "            self.REWARD_MEMORY = torch.zeros(max_memory_size, 1)\n",
        "            self.DONE_MEMORY = torch.zeros(max_memory_size, 1)\n",
        "            self.terminal_position = 0\n",
        "            self.num_in_queue = 0\n",
        "\n",
        "        self.STATE_MEMORY = torch.zeros(max_memory_size, *self.state_space)\n",
        "        self.STATE2_MEMORY = torch.zeros(max_memory_size, *self.state_space)\n",
        "        self.memory_sample_size = batch_size\n",
        "\n",
        "        self.gamma = gamma\n",
        "        # Use Huber loss function\n",
        "        self.l1 = nn.SmoothL1Loss().to(self.device)\n",
        "        self.exploration_max = exploration_max\n",
        "        self.exploration_rate = exploration_max\n",
        "        self.exploration_min = exploration_min\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "    # Store data to experience pool\n",
        "    def remember(self, state, action, reward, state2, done):\n",
        "        self.STATE_MEMORY[self.terminal_position] = state.float()\n",
        "        self.ACTION_MEMORY[self.terminal_position] = action.float()\n",
        "        self.REWARD_MEMORY[self.terminal_position] = reward.float()\n",
        "        self.STATE2_MEMORY[self.terminal_position] = state2.float()\n",
        "        self.DONE_MEMORY[self.terminal_position] = done.float()\n",
        "        self.terminal_position = (self.terminal_position + 1) % self.max_memory_size\n",
        "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
        "\n",
        "    def get_batch_experiences(self):\n",
        "        # Randomly batch size sample experiences\n",
        "        index = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
        "        STATE = self.STATE_MEMORY[index]\n",
        "        ACTION = self.ACTION_MEMORY[index]\n",
        "        REWARD = self.REWARD_MEMORY[index]\n",
        "        STATE2 = self.STATE2_MEMORY[index]\n",
        "        DONE = self.DONE_MEMORY[index]\n",
        "        return STATE, ACTION, REWARD, STATE2, DONE\n",
        "\n",
        "    def chosen_optimal_action(self, state):\n",
        "        optimal_action = None\n",
        "        if self.double_dqn:\n",
        "            # If it is DDQN, return the state input value local_net network to the action corresponding to the maximum network \n",
        "            # output value, and the network output position corresponds to the action position\n",
        "            optimal_action = torch.argmax(self.local_net(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
        "            return optimal_action\n",
        "        else:\n",
        "            # If it is DDQN, return the state input value dqn network to the action corresponding to the maximum output value \n",
        "            # of the network, and the network output position corresponds to the action position\n",
        "            optimal_action = torch.argmax(self.dqn(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
        "            return optimal_action\n",
        "\n",
        "    def chosen_action(self, state):\n",
        "        # Epsilon greedy policy\n",
        "        action_chosen = None\n",
        "        if self.double_dqn:\n",
        "            self.step += 1\n",
        "        # random exploration\n",
        "        if (np.random.rand() < self.exploration_rate):\n",
        "            # Randomly select an action from the action space and return\n",
        "            action_chosen = torch.tensor([[random.randrange(self.action_space)]])\n",
        "            return action_chosen\n",
        "        else:\n",
        "            action_chosen = self.chosen_optimal_action(state)\n",
        "            return action_chosen\n",
        "\n",
        "    def copy_model(self):\n",
        "        # Copy local_net weights to target_net every 5000 steps\n",
        "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
        "\n",
        "    def experience_replay(self):\n",
        "        if self.double_dqn and self.step % self.copy == 0:\n",
        "            # Copy local_net weights to target_net every 5000 steps\n",
        "            self.copy_model()\n",
        "\n",
        "        # If the experience pool is not full yet, return directly without training\n",
        "        if self.memory_sample_size > self.num_in_queue:\n",
        "            return\n",
        "        # After the experience pool is full, randomly obtain batch_size group data from the experience pool\n",
        "        STATE, ACTION, REWARD, STATE2, DONE = self.get_batch_experiences()\n",
        "\n",
        "        # Put the data on the GPU or cpu\n",
        "        STATE = STATE.to(self.device)\n",
        "        ACTION = ACTION.to(self.device)\n",
        "        REWARD = REWARD.to(self.device)\n",
        "        STATE2 = STATE2.to(self.device)\n",
        "        DONE = DONE.to(self.device)\n",
        "        # Optimizer gradient set to 0\n",
        "        self.optimizer.zero_grad()\n",
        "        if self.double_dqn:\n",
        "            #DDQN: Q*(S, A) <- r + gamma * max_a Q_target(S', a)  \n",
        "            target = REWARD + torch.mul((self.gamma * self.target_net(STATE2).max(1).values.unsqueeze(1)), 1 - DONE)\n",
        "            # Get the output current of the network\n",
        "            current = self.local_net(STATE).gather(1, ACTION.long())  # Local net approximation of Q-value\n",
        "        else:\n",
        "            # DQN: Q*(S, A) <- r + gamma * max_a Q(S', a)    \n",
        "            target = REWARD + torch.mul((self.gamma * self.dqn(STATE2).max(1).values.unsqueeze(1)), 1 - DONE)\n",
        "            # Get the output current of the network\n",
        "            current = self.dqn(STATE).gather(1, ACTION.long())\n",
        "        \n",
        "        # Huber loss as a loss function\n",
        "        loss = self.l1(current, target)\n",
        "        loss.backward()#gradients\n",
        "        self.optimizer.step()#Backpropagate\n",
        "        # Random exploration probability decay\n",
        "        self.exploration_rate = self.exploration_rate * self.exploration_decay\n",
        "        # 'exploration min' is always smaller or equal than epsilon\n",
        "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)\n",
        "\n",
        "def start(training_mode, pretrained, double_dqn, num_episodes=1000,actions=RIGHT_ONLY, exploration_max=1,save_path=\"./\",desc=\"\",render_mode=0):\n",
        "    '''\n",
        "        function: to train\n",
        "            parameter:\n",
        "                training_mode: True or False\n",
        "                    True: Indicates training mode\n",
        "                    False: non-training mode\n",
        "            pretrained: True or False\n",
        "                    True: Indicates that the trained model is loaded from save_path\n",
        "                    False: means not to load the trained model\n",
        "            double_dqn: True or False\n",
        "                    True: means use DDQN\n",
        "                    False: Indicates that DQN is used\n",
        "            num_episodes: int Set the number of games to loop\n",
        "            actions: RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT three action spaces\n",
        "            exploration_max: 0-1 The probability of initial random exploration, fixed at 1\n",
        "            save_path: folder path to save the model or load the model\n",
        "            desc: training display progress bar information\n",
        "            render_mode: The mode to display the test results, 0 means that the py file is tested locally, and 1 means that ipynb is tested in colab\n",
        "    '''\n",
        "    # Load 1-1 game environment object from gym_super_mario_bros library\n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "    # Process the obtained game scene\n",
        "    # Wraps the environment so that frames are grayscale\n",
        "    env = MSE(env)\n",
        "    env = MR84x84(env)\n",
        "    env = imgToTorch(env)\n",
        "    env = BW(env, 4)\n",
        "    env = PixelNormalization(env)\n",
        "    env = JoypadSpace(env, actions)\n",
        "\n",
        "    # Game obtains the input space size After the previous setting of the game environment, the game state is compressed into an array size of \n",
        "    # four consecutive frames: (4,84,84)\n",
        "    observation_space = env.observation_space.shape\n",
        "    # Game action space is the number of button combinations, len(RIGHT_ONLY)\n",
        "    action_space = env.action_space.n\n",
        "\n",
        "    agent = DQN_agent(state_space=observation_space,\n",
        "                     action_space=action_space,\n",
        "                     max_memory_size=30000, # Experience pool storage length\n",
        "                     batch_size=32, # Draw 32 from the experience pool each time as a batch\n",
        "                     gamma=0.90, # Indicates the importance of future rewards, the greater the value of future rewards\n",
        "                     lr=0.00025, # Network training learning rate\n",
        "                     dropout=0.2,  # Prevent overfitting The network does not use this parameter\n",
        "                     exploration_max=1.0, # Random action to explore the game space, the initial value is 1\n",
        "                     exploration_min=0.02, # Random action to explore the game space The final value after decay, indicating that there is a probability of 0.02 random action at the end\n",
        "                     exploration_decay=0.99, # Exploring Probability Decay Factors\n",
        "                     double_dqn=double_dqn,\n",
        "                     pretrained=pretrained,\n",
        "                     save_path=save_path)\n",
        "\n",
        "    # Reset the game for each episode\n",
        "    num_episodes = num_episodes # How many games to train\n",
        "    env.reset()\n",
        "\n",
        "    rewards_total = [] # Store rewards earned for each game\n",
        "    average_rewards_total={} # store the average reward\n",
        "    if training_mode and pretrained: # If you continue training, the previously saved rewards_total average_rewards_total will be loaded\n",
        "        with open(os.path.join(save_path,\"rewards_total.pkl\"), 'rb') as f:\n",
        "            rewards_total = pickle.load(f)\n",
        "        with open(os.path.join(save_path,\"average_rewards_total.pkl\"), 'rb') as f:\n",
        "            average_rewards_total = pickle.load(f)\n",
        "\n",
        "    for ep_num in tqdm(range(num_episodes),desc=desc): # Circuit training Show training progress bar\n",
        "        state = env.reset()   # Game environment initialization Return to the scene when the game is initialized, it is a numpy matrix of 4*84*84\n",
        "        state = torch.Tensor([state])\n",
        "        reward_total = 0    # The total reward is because the reward of a certain step returned by the gym every time is accumulated as the total reward\n",
        "        steps = 0    # The number of acts performed\n",
        "        while True:\n",
        "            if not training_mode: # If it's not training, show the Mario game screen\n",
        "                if render_mode==0:\n",
        "                    env.render()\n",
        "                elif render_mode==1:\n",
        "                    plt.figure(3)\n",
        "                    plt.clf()\n",
        "                    plt.imshow(env.render(mode='rgb_array'))\n",
        "                    plt.title(\"Episode: %d\" % (ep_num))\n",
        "                    plt.axis('off')\n",
        "                    display.clear_output(wait=True)\n",
        "                    display.display(plt.gcf())\n",
        "\n",
        "            action = agent.chosen_action(state)  # Get the actions that should be performed in the scene from the DQN_agent\n",
        "            steps = steps + 1    # Increase the number of actions by 1\n",
        "            next_state, reward, terminal, info = env.step(int(action[0])) # Get the updated state of the game after executing this act from the gym environment\n",
        "            reward_total = reward_total + reward\n",
        "            next_state = torch.Tensor([next_state])\n",
        "            reward = torch.tensor([reward]).unsqueeze(0)\n",
        "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
        "            if training_mode:\n",
        "                agent.remember(state, action, reward, next_state, terminal) # Put this result into the experience pool\n",
        "                agent.experience_replay() # Experience Playback Training\n",
        "            state = next_state\n",
        "            if terminal: # Game over\n",
        "                break\n",
        "        rewards_total.append(reward_total)\n",
        "\n",
        "        if ep_num % 100 == 0 and ep_num != 0: # Save the model and draw every n games\n",
        "            print(\"Episode {} score = {}, average score = {}\".format(ep_num + 1, rewards_total[-1],\n",
        "                                                                     np.mean(rewards_total)))\n",
        "\n",
        "            if training_mode:\n",
        "                # Drawing\n",
        "                average_rewards_total[ep_num] = np.mean(rewards_total)\n",
        "                plt.title(\"episodes & average reward (%s)\" % (\"DDQN\" if double_dqn else \"DQN\"))\n",
        "                plt.plot(list(average_rewards_total.keys()), list(average_rewards_total.values()))\n",
        "                plt.savefig(os.path.join(save_path, \"average_reward_plot.jpg\"))\n",
        "                plt.clf()\n",
        "                with open(os.path.join(save_path,\"terminal_position.pkl\"), \"wb\") as f:\n",
        "                    pickle.dump(agent.terminal_position, f)\n",
        "                with open(os.path.join(save_path,\"num_in_queue.pkl\"), \"wb\") as f:\n",
        "                    pickle.dump(agent.num_in_queue, f)\n",
        "                with open(os.path.join(save_path,\"rewards_total.pkl\"), \"wb\") as f:\n",
        "                    pickle.dump(rewards_total, f)\n",
        "                with open(os.path.join(save_path, \"average_rewards_total.pkl\"), \"wb\") as f:\n",
        "                    pickle.dump(rewards_total, f)\n",
        "\n",
        "                # Save network parameters\n",
        "                if agent.double_dqn:\n",
        "                    torch.save(agent.local_net.state_dict(), os.path.join(save_path,\"DQN1.pt\"))\n",
        "                    torch.save(agent.target_net.state_dict(), os.path.join(save_path,\"DQN2.pt\"))\n",
        "                else:\n",
        "                    torch.save(agent.dqn.state_dict(), os.path.join(save_path,\"DQN.pt\"))\n",
        "                torch.save(agent.ACTION_MEMORY, os.path.join(save_path,\"ACTION_MEMORY.pt\"))\n",
        "                torch.save(agent.REWARD_MEMORY, os.path.join(save_path,\"REWARD_MEMORY.pt\"))\n",
        "                torch.save(agent.DONE_MEMORY, os.path.join(save_path,\"DONE_MEMORY.pt\"))\n",
        "\n",
        "        num_episodes += 1\n",
        "\n",
        "    print(\"Episode {} score = {}, average score = {}\".format(ep_num + 1, rewards_total[-1], np.mean(rewards_total)))\n",
        "\n",
        "    # Save the trained memory so that we can continue from where we stop using 'pretrained' = True\n",
        "    if training_mode:\n",
        "        average_rewards_total[ep_num+1] = np.mean(rewards_total)\n",
        "        plt.title(\"episodes & average reward (%s)\"%(\"DDQN\" if double_dqn else \"DQN\"))\n",
        "        plt.plot(list(average_rewards_total.keys()), list(average_rewards_total.values()))\n",
        "        plt.savefig(os.path.join(save_path, \"average_reward_plot.jpg\"))\n",
        "        plt.clf()\n",
        "        with open(os.path.join(save_path, \"terminal_position.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(agent.terminal_position, f)\n",
        "        with open(os.path.join(save_path, \"num_in_queue.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(agent.num_in_queue, f)\n",
        "        with open(os.path.join(save_path, \"rewards_total.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(rewards_total, f)\n",
        "        with open(os.path.join(save_path, \"average_rewards_total.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(rewards_total, f)\n",
        "        # Save network parameters\n",
        "        if agent.double_dqn:\n",
        "            torch.save(agent.local_net.state_dict(), os.path.join(save_path, \"DQN1.pt\"))\n",
        "            torch.save(agent.target_net.state_dict(), os.path.join(save_path, \"DQN2.pt\"))\n",
        "        else:\n",
        "            torch.save(agent.dqn.state_dict(), os.path.join(save_path, \"DQN.pt\"))\n",
        "        torch.save(agent.ACTION_MEMORY, os.path.join(save_path, \"ACTION_MEMORY.pt\"))\n",
        "        torch.save(agent.REWARD_MEMORY, os.path.join(save_path, \"REWARD_MEMORY.pt\"))\n",
        "        torch.save(agent.DONE_MEMORY, os.path.join(save_path, \"DONE_MEMORY.pt\"))\n",
        "    env.close()\n",
        "    # Return the average reward situation, used for DDQN and DQN cooperative drawing comparison\n",
        "    return average_rewards_total\n",
        "\n",
        "\n",
        "def create_save_path(folder):\n",
        "    '''\n",
        "      Role: create a storage folder\n",
        "      introduce:\n",
        "          If the folder is a multi-level folder that does not exist, it will be created recursively at one time\n",
        "          And create a run folder in the folder directory. If the run folder exists, the run1 folder will be created, and if the run1 folder exists, the run2 folder will be created....\n",
        "     '''\n",
        "     \n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "    p1=os.path.join(folder, \"run\")\n",
        "    if  not os.path.exists(p1):\n",
        "        os.mkdir(p1)\n",
        "        return p1\n",
        "    else:\n",
        "        n=1\n",
        "        p1=os.path.join(folder,\"run%d\"%n)\n",
        "        while os.path.exists(p1):\n",
        "            n+=1\n",
        "            p1=os.path.join(folder,\"run%d\"%n)\n",
        "        os.mkdir(p1)\n",
        "        return p1\n",
        "\n",
        "\n",
        "def train_actions(to_actions=None,save_folder=\"checkpoints\", num_episodes=10000):\n",
        "    # actions={\"RIGHT_ONLY\":RIGHT_ONLY}\n",
        "    if to_actions is None:\n",
        "        to_actions = {\"RIGHT_ONLY\":RIGHT_ONLY}\n",
        "    run_path=create_save_path(save_folder)\n",
        "    for actions_name,actions in to_actions.items():\n",
        "        dqn_save_path=os.path.join(run_path,actions_name+\"_dqn\")\n",
        "        ddqn_save_path=os.path.join(run_path,actions_name+\"_ddqn\")\n",
        "        os.mkdir(dqn_save_path)\n",
        "        os.mkdir(ddqn_save_path)\n",
        "        dqn_average_rewards_total=start(training_mode=True, pretrained=False, double_dqn=False, num_episodes=num_episodes, actions=actions,exploration_max=1,save_path=dqn_save_path,desc=actions_name+\" dqn\")\n",
        "        ddqn_average_rewards_total=start(training_mode=True, pretrained=False, double_dqn=True, num_episodes=num_episodes, actions=actions,exploration_max=1,save_path=ddqn_save_path,desc=actions_name+\" ddqn\")\n",
        "        plt.title(\"episodes & average reward(%s)\"%actions_name)\n",
        "        plt.plot(list(ddqn_average_rewards_total.keys()), list(ddqn_average_rewards_total.values()), list(dqn_average_rewards_total.keys()), list(dqn_average_rewards_total.values()), \"y-\")\n",
        "        plt.legend([\"DDQN\", \"DQN\"], loc='best')\n",
        "        plt.savefig(os.path.join(run_path,\"DDQNvsDQN\"))\n",
        "        plt.clf()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wOWQEgqXoYn6",
        "outputId": "cae58191-9c36-495b-da26-09b428a9ed01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:   2%|▏         | 101/5000 [06:35<8:33:48,  6.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 101 score = 1332.0, average score = 591.4851485148515\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:   4%|▍         | 201/5000 [12:33<3:49:04,  2.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 201 score = 1340.0, average score = 639.1641791044776\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:   6%|▌         | 301/5000 [18:50<9:30:05,  7.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 301 score = 1310.0, average score = 605.8338870431894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:   8%|▊         | 401/5000 [25:36<7:43:27,  6.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 401 score = 701.0, average score = 631.7306733167082\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  10%|█         | 501/5000 [31:10<2:57:23,  2.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 501 score = 566.0, average score = 627.6966067864272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  12%|█▏        | 601/5000 [35:25<3:09:23,  2.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 601 score = 251.0, average score = 605.5906821963395\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  14%|█▍        | 701/5000 [43:49<5:31:46,  4.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 701 score = 252.0, average score = 585.0599144079886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  16%|█▌        | 801/5000 [48:37<5:02:25,  4.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 801 score = 1325.0, average score = 582.7977528089888\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  18%|█▊        | 901/5000 [54:47<5:20:22,  4.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 901 score = 526.0, average score = 589.0221975582685\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  20%|██        | 1001/5000 [59:04<1:19:54,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1001 score = 248.0, average score = 573.6493506493506\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  22%|██▏       | 1101/5000 [1:03:56<5:31:20,  5.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1101 score = 739.0, average score = 564.960944595822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  24%|██▍       | 1201/5000 [1:09:55<3:35:22,  3.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1201 score = 243.0, average score = 573.2306411323897\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  26%|██▌       | 1301/5000 [1:16:41<5:51:50,  5.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1301 score = 235.0, average score = 568.3774019984627\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  28%|██▊       | 1401/5000 [1:21:35<3:15:32,  3.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1401 score = 252.0, average score = 553.9628836545324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  30%|███       | 1501/5000 [1:27:01<2:38:01,  2.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1501 score = 240.0, average score = 557.599600266489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  32%|███▏      | 1601/5000 [1:33:16<3:24:39,  3.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1601 score = 248.0, average score = 563.8532167395377\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  34%|███▍      | 1701/5000 [1:41:42<2:42:57,  2.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1701 score = 609.0, average score = 563.6713697824808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  36%|███▌      | 1801/5000 [1:49:05<3:54:52,  4.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1801 score = 1315.0, average score = 573.88228761799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  38%|███▊      | 1901/5000 [1:56:04<37:03,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1901 score = 252.0, average score = 575.0394529195161\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  40%|████      | 2001/5000 [2:02:44<2:12:50,  2.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2001 score = 594.0, average score = 576.775612193903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  42%|████▏     | 2101/5000 [2:07:56<2:24:35,  2.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2101 score = 236.0, average score = 579.5797239409804\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  44%|████▍     | 2201/5000 [2:15:10<3:27:30,  4.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2201 score = 1338.0, average score = 584.1558382553385\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  46%|████▌     | 2301/5000 [2:21:37<4:46:27,  6.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2301 score = 1344.0, average score = 592.0560625814863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  48%|████▊     | 2401/5000 [2:29:58<11:22:43, 15.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2401 score = 139.0, average score = 598.143273635985\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  50%|█████     | 2501/5000 [2:36:08<2:27:40,  3.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2501 score = 766.0, average score = 602.1639344262295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  52%|█████▏    | 2601/5000 [2:43:12<1:58:31,  2.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2601 score = 1046.0, average score = 607.2568242983468\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  54%|█████▍    | 2701/5000 [2:52:23<1:28:26,  2.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2701 score = 243.0, average score = 615.5205479452055\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  56%|█████▌    | 2801/5000 [2:58:24<2:05:04,  3.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2801 score = 641.0, average score = 622.6394144948233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  58%|█████▊    | 2901/5000 [3:06:52<2:15:30,  3.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2901 score = 605.0, average score = 627.7959324370906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  60%|██████    | 3001/5000 [3:14:17<2:51:25,  5.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3001 score = 768.0, average score = 630.6461179606798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  62%|██████▏   | 3101/5000 [3:20:59<1:57:00,  3.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3101 score = 240.0, average score = 633.3437600773943\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  64%|██████▍   | 3201/5000 [3:26:37<3:10:29,  6.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3201 score = 1214.0, average score = 639.3017806935333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  66%|██████▌   | 3301/5000 [3:31:52<1:27:50,  3.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3301 score = 1438.0, average score = 647.6498030899727\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  68%|██████▊   | 3401/5000 [3:38:19<1:08:56,  2.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3401 score = 1046.0, average score = 656.0720376359894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  70%|███████   | 3501/5000 [3:42:37<1:03:40,  2.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3501 score = 248.0, average score = 662.9608683233362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  72%|███████▏  | 3601/5000 [3:47:13<58:11,  2.50s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3601 score = 610.0, average score = 668.6398222715912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  74%|███████▍  | 3701/5000 [3:51:24<38:05,  1.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3701 score = 1438.0, average score = 670.0437719535261\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  76%|███████▌  | 3801/5000 [3:55:10<41:17,  2.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3801 score = 622.0, average score = 670.4980268350434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  78%|███████▊  | 3901/5000 [3:58:53<43:09,  2.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3901 score = 587.0, average score = 672.190720328121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  80%|████████  | 4001/5000 [4:02:42<32:23,  1.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4001 score = 731.0, average score = 672.246688327918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  82%|████████▏ | 4101/5000 [4:08:09<37:42,  2.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4101 score = 610.0, average score = 677.1419166057059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  84%|████████▍ | 4201/5000 [4:15:21<58:53,  4.42s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4201 score = 1434.0, average score = 685.5198762199476\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  86%|████████▌ | 4301/5000 [4:20:11<32:46,  2.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4301 score = 1910.0, average score = 690.330388281795\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  88%|████████▊ | 4401/5000 [4:24:53<41:42,  4.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4401 score = 613.0, average score = 695.6916609861395\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  90%|█████████ | 4501/5000 [4:29:46<23:58,  2.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4501 score = 614.0, average score = 699.5934236836258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  92%|█████████▏| 4601/5000 [4:34:23<14:17,  2.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4601 score = 608.0, average score = 701.0304281677896\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  94%|█████████▍| 4701/5000 [4:38:14<10:52,  2.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4701 score = 605.0, average score = 705.1006168900234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  96%|█████████▌| 4801/5000 [4:43:34<08:20,  2.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4801 score = 602.0, average score = 707.6440324932306\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn:  98%|█████████▊| 4901/5000 [4:47:04<03:05,  1.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4901 score = 637.0, average score = 708.0057131197715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY dqn: 100%|██████████| 5000/5000 [4:51:35<00:00,  3.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 5000 score = 638.0, average score = 708.714\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:   2%|▏         | 101/5000 [04:24<3:56:51,  2.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 101 score = 1432.0, average score = 694.8910891089109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:   4%|▍         | 201/5000 [08:47<3:41:22,  2.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 201 score = 237.0, average score = 670.3781094527363\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:   6%|▌         | 301/5000 [12:27<2:16:04,  1.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 301 score = 239.0, average score = 647.1495016611295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:   8%|▊         | 401/5000 [15:28<3:48:01,  2.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 401 score = 1035.0, average score = 630.5137157107232\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  10%|█         | 501/5000 [20:32<2:42:10,  2.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 501 score = 1049.0, average score = 665.8003992015969\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  12%|█▏        | 600/5000 [24:06<1:53:50,  1.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 601 score = 606.0, average score = 679.7953410981697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  14%|█▍        | 701/5000 [28:40<2:37:59,  2.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 701 score = 1049.0, average score = 717.7974322396576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  16%|█▌        | 801/5000 [33:51<3:16:46,  2.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 801 score = 1036.0, average score = 760.1148564294632\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  18%|█▊        | 901/5000 [39:25<2:13:50,  1.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 901 score = 610.0, average score = 789.3085460599334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  20%|██        | 1001/5000 [44:59<3:10:01,  2.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1001 score = 1049.0, average score = 825.6603396603397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  22%|██▏       | 1101/5000 [51:26<3:21:30,  3.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1101 score = 1050.0, average score = 853.3696639418711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  24%|██▍       | 1201/5000 [56:06<2:49:49,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1201 score = 604.0, average score = 866.3755203996669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  26%|██▌       | 1301/5000 [1:01:50<3:09:35,  3.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1301 score = 724.0, average score = 886.5918524212144\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  28%|██▊       | 1401/5000 [1:06:34<2:42:40,  2.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1401 score = 611.0, average score = 897.6845110635261\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  30%|███       | 1501/5000 [1:11:09<2:56:34,  3.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1501 score = 623.0, average score = 907.1045969353764\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  32%|███▏      | 1601/5000 [1:17:33<2:14:43,  2.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1601 score = 621.0, average score = 919.6689569019363\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  34%|███▍      | 1701/5000 [1:22:34<2:33:50,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1701 score = 1050.0, average score = 932.440329218107\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  36%|███▌      | 1801/5000 [1:28:33<2:22:04,  2.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1801 score = 607.0, average score = 946.3792337590228\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  38%|███▊      | 1901/5000 [1:33:41<1:53:55,  2.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1901 score = 610.0, average score = 950.7716991057338\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  40%|████      | 2001/5000 [1:39:02<1:24:51,  1.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2001 score = 232.0, average score = 955.2863568215892\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  42%|████▏     | 2101/5000 [1:43:10<1:44:41,  2.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2101 score = 610.0, average score = 955.1508805330794\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  44%|████▍     | 2201/5000 [1:47:14<1:58:40,  2.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2201 score = 1434.0, average score = 956.0458882326216\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  46%|████▌     | 2301/5000 [1:51:55<2:41:53,  3.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2301 score = 1040.0, average score = 964.4589308996088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  48%|████▊     | 2401/5000 [1:58:22<2:07:57,  2.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2401 score = 1436.0, average score = 972.1936693044564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  50%|█████     | 2501/5000 [2:03:20<2:33:44,  3.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2501 score = 1846.0, average score = 982.0703718512596\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  52%|█████▏    | 2601/5000 [2:09:00<2:15:09,  3.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2601 score = 1437.0, average score = 997.0073048827375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  54%|█████▍    | 2701/5000 [2:15:01<2:08:43,  3.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2701 score = 1344.0, average score = 1011.1873380229545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  56%|█████▌    | 2801/5000 [2:22:52<3:21:44,  5.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2801 score = 611.0, average score = 1031.8450553373796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  58%|█████▊    | 2901/5000 [2:30:56<2:00:59,  3.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2901 score = 605.0, average score = 1052.3147190623922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  60%|██████    | 3001/5000 [2:39:58<3:31:22,  6.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3001 score = 1859.0, average score = 1079.090303232256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  62%|██████▏   | 3101/5000 [2:47:23<2:07:21,  4.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3101 score = 2355.0, average score = 1098.2099322799097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  64%|██████▍   | 3201/5000 [2:55:07<2:16:18,  4.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3201 score = 2363.0, average score = 1116.9203373945643\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  66%|██████▌   | 3301/5000 [3:03:30<2:14:55,  4.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3301 score = 1695.0, average score = 1139.1075431687368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  68%|██████▊   | 3401/5000 [3:11:25<1:38:33,  3.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3401 score = 1698.0, average score = 1154.5607174360482\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  70%|███████   | 3501/5000 [3:19:38<1:43:13,  4.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3501 score = 1352.0, average score = 1171.4278777492145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  72%|███████▏  | 3601/5000 [3:28:29<1:26:14,  3.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3601 score = 1351.0, average score = 1191.4243265759512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  74%|███████▍  | 3701/5000 [3:38:30<1:56:38,  5.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3701 score = 1733.0, average score = 1215.5598486895433\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  76%|███████▌  | 3801/5000 [3:47:35<1:44:47,  5.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3801 score = 1713.0, average score = 1238.3375427519074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  78%|███████▊  | 3901/5000 [3:57:27<1:12:04,  3.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 3901 score = 1881.0, average score = 1253.1750833119713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  80%|████████  | 4001/5000 [4:07:58<2:41:32,  9.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4001 score = 3052.0, average score = 1271.9467633091726\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  82%|████████▏ | 4101/5000 [4:16:53<59:53,  4.00s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4101 score = 1061.0, average score = 1288.3313825896123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  84%|████████▍ | 4200/5000 [4:29:26<2:09:56,  9.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4201 score = 3062.0, average score = 1310.7688645560581\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  86%|████████▌ | 4301/5000 [4:39:06<1:02:04,  5.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4301 score = 2367.0, average score = 1326.8272494768657\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  88%|████████▊ | 4401/5000 [4:47:34<41:43,  4.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4401 score = 618.0, average score = 1341.5178368552602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  90%|█████████ | 4501/5000 [4:57:26<30:51,  3.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4501 score = 1930.0, average score = 1362.4970006665185\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  92%|█████████▏| 4601/5000 [5:07:17<27:30,  4.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4601 score = 1881.0, average score = 1375.7933058030862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  94%|█████████▍| 4701/5000 [5:17:14<1:25:31, 17.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4701 score = 1771.0, average score = 1393.2163369495852\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  96%|█████████▌| 4801/5000 [5:26:23<14:29,  4.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4801 score = 1691.0, average score = 1407.3276400749844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn:  98%|█████████▊| 4901/5000 [5:34:32<05:43,  3.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 4901 score = 1707.0, average score = 1420.2721893491125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RIGHT_ONLY ddqn: 100%|██████████| 5000/5000 [5:43:45<00:00,  4.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 5000 score = 2371.0, average score = 1434.3166\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "######train#####\n",
        "#actions is a list of action types that need to be trained. If you train others, add \"COMPLEX_MOVEMENT\":COMPLEX_MOVEMENT and \"RIGHT_ONLY\":RIGHT_ONLY, and each type of dqn and ddqn in the list will be trained in turn\n",
        "#save_fodel is the folder where the training results are saved. If it does not exist, it will be recursively created. Each training will generate a run folder, and multiple times will generate run1, run2, run3...\n",
        "\n",
        "actions={\"RIGHT_ONLY\":RIGHT_ONLY}\n",
        "save_fodel=\"drive/MyDrive/DQN_DDQN/RIGHT_ONLY_checkpoints\"\n",
        "train_actions(to_actions=actions,save_folder=save_fodel,num_episodes=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "CsxyjrexoeLW",
        "outputId": "7a6c4030-b08a-4a29-b1e0-2185d654fe64"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:24<00:00, 24.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1 score = 1316.0, average score = 1316.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "######test######\n",
        "# save_root The run folder path after training\n",
        "# The models in the actions folder are suitable for that type of action\n",
        "# satrt parameter\n",
        "#       training_mode=False, pretrained=True means testing\n",
        "#       double_dqn: True if the model in the folder is DDQN and False if DQN\n",
        "#       num_episodes: set the number of tests, do not set too large in colab, it is easy to collapse the memory\n",
        "#       render_mode: display mode, if it is a py file for local testing, it is 0, and ipynb is 1\n",
        "#       exploration_max: The probability of performing random actions can be set to a very small number during the test\n",
        "#######Example####\n",
        "save_root=\"drive/MyDrive/DQN_DDQN/RIGHT_ONLY_checkpoints/run1/RIGHT_ONLY_ddqn\"\n",
        "actions=RIGHT_ONLY\n",
        "start(training_mode=False, pretrained=True, double_dqn=True, num_episodes=1, render_mode=1,actions=actions,exploration_max=0.05, save_path=save_root, desc=\"\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}