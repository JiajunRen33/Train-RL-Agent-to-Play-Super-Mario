{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#DQN&DDQN_SIMPLEMOVE.ipynb trains the agent 5000 times in simple move actions with DQN and DDQN algorithms."
      ],
      "metadata": {
        "id": "cdUinBRyevRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F53Es-JWmv63",
        "outputId": "d1baaa45-5d14-46a3-a8d2-99014fc53990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC6GDXjdnMz9",
        "outputId": "665d7cc5-4705-4315-a492-224a910ea733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym-super-mario-bros==7.4.0\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "Collecting nes-py>=8.1.4\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.21.6)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.64.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.10.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py): started\n",
            "  Building wheel for nes-py (setup.py): finished with status 'done'\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp38-cp38-linux_x86_64.whl size=439709 sha256=d8fbae9ac2f6638abf3f399466987692a9ef95b18c4f93f58b602a29dbb9a93c\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/e5/5c/8dfae61b44dbf56c458483aa09accef55a650e0527f6cbd872\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install gym-super-mario-bros==7.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab48E-rZoI_t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "import gym\n",
        "import numpy as np\n",
        "import collections\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "class MSE(gym.Wrapper):\n",
        "    \"\"\"\n",
        "        Adjust the game screen output of the Mario game, because when playing the game, \n",
        "        the continuity of the game will make most of the continuous pictures obtained \n",
        "        within a period of time approximate. In order to reduce the input of approximate \n",
        "        game scene pictures and improve training efficiency, it will Traverse the skip \n",
        "        frame image, calculate the sum of the skip frame image rewards, and perform a \n",
        "        maximum pooling in the last 2 frames of the skip frame, and generate a frame \n",
        "        image after pooling to represent the skip frame image. At here, skip=4.\n",
        "    \"\"\"\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MSE, self).__init__(env)\n",
        "        #for the most recent raw observation, aka for max pooling across time steps\n",
        "        self.buffer_observation = collections.deque(maxlen=2)\n",
        "        self.skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        reward_total = 0.0\n",
        "        for _ in range(self.skip):\n",
        "            observation, reward, done, info = self.env.step(action)\n",
        "            self.buffer_observation.append(observation)\n",
        "            reward_total = reward_total + reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self.buffer_observation), axis=0)\n",
        "        return max_frame, reward_total, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Skip frames, and max pooling is implemented here\"\"\"\n",
        "        self.buffer_observation.clear()\n",
        "        observation = self.env.reset()\n",
        "        self.buffer_observation.append(observation)\n",
        "        return observation\n",
        "\n",
        "class MR84x84(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Set the size of the game screen Adjust the size of the game screen \n",
        "    to 84*84 grayscale image, ie shape 84,84,1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env=None):\n",
        "        super(MR84x84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return MR84x84.process(obs)\n",
        "\n",
        "    def process(frame):\n",
        "        if frame.size == 240 * 256 * 3:\n",
        "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "            # image normalization\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "class imgToTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Convert 84*84*1 array to 1*84*84 tensor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(imgToTorch, self).__init__(env)\n",
        "        original_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(original_shape[-1], original_shape[0], original_shape[1]),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "class BW(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Function: return the picture of consecutive n_steps frames (after skip)\n",
        "    Implementation: When env calls the reset() method, self.buffer will be initialized, \n",
        "    and its size will be initialized to n_steps*84*84, that is, four 1*84*84 game screens \n",
        "    can be placed. When initializing, the first three It is all 0, the last one is the initialized game screen\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BW, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        # self.env.reset() is the screen where the game starts as a parameter of self.observation\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # Move the last three positions forward, the first position will be covered\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        # Put observation to the last position\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "class PixelNormalization(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Normalize 4*84*84 data so that the pixel values ​​are distributed between 0-1, a common \n",
        "    operation of convolutional neural networks\n",
        "    \"\"\"\n",
        "\n",
        "    def observation(self, obs):\n",
        "        normalization = np.array(obs).astype(np.float32) / 255.0\n",
        "        return normalization\n",
        "\n",
        "'''\n",
        "The above classes are all inherited from the classes in gym\n",
        "'''\n",
        "\n",
        "class DQN_network(nn.Module):\n",
        "    \"\"\"\n",
        "        The most primitive Q-learning algorithm always needs a Q table to record during the execution \n",
        "        process. When the dimension is not high, the Q table can still meet the demand, but when encountering \n",
        "        exponential dimension, the efficiency of the Q table is very high. limited. Therefore, we consider a \n",
        "        value function approximation method, so that each time we only need to know S or A in advance, we can \n",
        "        get the corresponding Q value in real time.\n",
        "\n",
        "        Originally, the Q value was found through S (state) and A (action), but now a neural network is used to \n",
        "        obtain an approximate Q value. The neural network is very powerful, so we can directly use the game screen\n",
        "        as a state S and input it into the network. We can use the output of the network as the Q value, and make \n",
        "        each output correspond to the Q value of an action A, so that we can Get the Q value of each action in state S. \n",
        "        This is DQN.\n",
        "\n",
        "        For this Mario game, if we only look at a certain frame alone, we must be missing some relevant information. \n",
        "        For example, we cannot judge whether Mario is rising or falling through one frame. So it is estimated that if \n",
        "        several consecutive frames are input at the same time, the neural network can know whether Mario is rising or \n",
        "        falling, and it can be estimated that this will be better as an input. This is why the first few classes that \n",
        "        inherit from gym have transformed Mario's output game screen.\n",
        "\n",
        "    This class is the construction of the convolutional neural network. The input of the convolutional neural network \n",
        "    is an 84*84*4 game scene, and the output is action_space, corresponding to the Q value of each action.    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN_network, self).__init__()\n",
        "        # Build convolutional layers\n",
        "        self.conv = nn.Sequential(\n",
        "            # input_shape[0] The number of channels of the input image, the superposition of 4 single-channel images is 4\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Get the output of the convolutional layer and expand it into 1 dimension, then input the size of the fully connected layer\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "\n",
        "        # Build a fully connected layer\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)\n",
        "\n",
        "class DQN_agent:\n",
        "    def __init__(self, state_space, action_space, max_memory_size, batch_size, gamma, lr,\n",
        "                 dropout, exploration_max, exploration_min, exploration_decay, double_dqn, pretrained,save_path):\n",
        "        '''\n",
        "            Initialization parameters:\n",
        "                1. state_space: game state space, that is, the shape (4,84,84) of the game state obtained after processing the env screen above represents four consecutive game scenes\n",
        "                2.action_space: game action space, that is, the number of actions RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT corresponds to 5 7 12\n",
        "                3.max_memory_size: The size of the experience playback pool\n",
        "                    The concept and purpose of experience playback: Reinforcement learning has stability problems due to the correlation between states, because the samples collected by \n",
        "                    the agent when exploring the environment are a time series, and there is continuity between the samples, so it is necessary to break the time correlation , the solution \n",
        "                    is to store the current training state to the memory M during training, and randomly sample mini-batches from M for updating when updating parameters.\n",
        "                    Specifically, the data type stored in M is <s,a,r,s′,done>:\n",
        "                        a: action performed by state s\n",
        "                        s: state s before the execution of state a\n",
        "                        s′: the new state s after state a is executed\n",
        "                        r: the reward obtained by executing a in state s\n",
        "                        done: whether the game is over\n",
        "                    M has a maximum length limit to ensure that the updated data are all recent data. The ultimate goal of experience replay is to prevent overfitting.\n",
        "                4. batch_size: the batch of each training, that is, each time the batch_size group data is randomly selected from the experience pool for training\n",
        "                5. Gamma: 0-1\n",
        "                    ddqn according to Q*(S, A) <- r + gamma * max_a Q_target(S', a)\n",
        "                    dqn according to Q*(S, A) <- r + gamma * max_a Q(S', a)\n",
        "                    It can be seen that the Q value is the sum of the current reward and the predicted Q value, and gamma indicates the value of the future\n",
        "                6.lr: learning rate\n",
        "                7. exploration_max: The probability of random exploration is the probability of taking random actions\n",
        "                    Purpose: The initial network has not been trained, directly using the network output will cause the same scene to be stuck directly, so\n",
        "                             Use random actions to obtain training data and explore the state space\n",
        "                8.exploration_min: The lowest probability of random exploration. With the training of the network, the network will become better and better, and the space for exploration will become more and more, so\n",
        "                    The probability of using random exploration needs to be slowly reduced until it falls to the preset minimum value\n",
        "                9.exploration_decay: random exploration decay factor, now_exploration=exploration*exploration_decay, is a value between 0-1, less than 1\n",
        "                10.double_dqn: the agent type is DDQN or DQN, the optimization goals of the two are different\n",
        "                11.pretrained: whether it is a test, if it is a test, just test, no training is required\n",
        "                12.save_path: test model loading path\n",
        "        '''\n",
        "        # Define DQN Layers\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.double_dqn = double_dqn\n",
        "        self.pretrained = pretrained\n",
        "        # If there is a GPU, use GPU training, otherwise CPU\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Double DQN network\n",
        "        if self.double_dqn:\n",
        "            # Use Net to create two neural networks: evaluation network and target network\n",
        "            self.local_net = DQN_network(state_space, action_space).to(self.device)\n",
        "            self.target_net = DQN_network(state_space, action_space).to(self.device)\n",
        "            # If it is a test, load the trained model from memory\n",
        "            if self.pretrained:\n",
        "                self.local_net.load_state_dict(torch.load(os.path.join(save_path,\"DQN1.pt\"), map_location=torch.device(self.device)))\n",
        "                self.target_net.load_state_dict(torch.load(os.path.join(save_path,\"DQN2.pt\"), map_location=torch.device(self.device)))\n",
        "            # Optimizer selects Adam\n",
        "            self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
        "            # #Copy local_net weights to target_net every 5000 steps\n",
        "            self.copy = 5000\n",
        "            self.step = 0\n",
        "        # DQN network\n",
        "        else:\n",
        "            # Only need to load a network\n",
        "            self.dqn = DQN_network(state_space, action_space).to(self.device)\n",
        "            if self.pretrained:\n",
        "                self.dqn.load_state_dict(torch.load(os.path.join(save_path,\"DQN.pt\"), map_location=torch.device(self.device)))\n",
        "            self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=lr)\n",
        "        # Create memory\n",
        "        self.max_memory_size = max_memory_size\n",
        "        #The entire experience pool is composed of STATE_MEMORY ACTION_MEMORY STATE2_MEMORY REWARD_MEMORY DONE_MEMORY, each of which is a torch tensor with a length of max_memory_size\n",
        "        #These five components (s, a, s', done)\n",
        "        if self.pretrained:\n",
        "            self.ACTION_MEMORY = torch.load(os.path.join(save_path,\"ACTION_MEMORY.pt\"))\n",
        "            self.REWARD_MEMORY = torch.load(os.path.join(save_path,\"REWARD_MEMORY.pt\"))\n",
        "            self.DONE_MEMORY = torch.load(os.path.join(save_path,\"DONE_MEMORY.pt\"))\n",
        "            with open(os.path.join(save_path,\"terminal_position.pkl\"), 'rb') as f:\n",
        "                self.terminal_position = pickle.load(f)\n",
        "            with open(os.path.join(save_path,\"num_in_queue.pkl\"), 'rb') as f:\n",
        "                self.num_in_queue = pickle.load(f)\n",
        "        else:\n",
        "            self.ACTION_MEMORY = torch.zeros(max_memory_size, 1)\n",
        "            self.REWARD_MEMORY = torch.zeros(max_memory_size, 1)\n",
        "            self.DONE_MEMORY = torch.zeros(max_memory_size, 1)\n",
        "            self.terminal_position = 0\n",
        "            self.num_in_queue = 0\n",
        "\n",
        "        self.STATE_MEMORY = torch.zeros(max_memory_size, *self.state_space)\n",
        "        self.STATE2_MEMORY = torch.zeros(max_memory_size, *self.state_space)\n",
        "        self.memory_sample_size = batch_size\n",
        "\n",
        "        self.gamma = gamma\n",
        "        # Use Huber loss function\n",
        "        self.l1 = nn.SmoothL1Loss().to(self.device)\n",
        "        self.exploration_max = exploration_max\n",
        "        self.exploration_rate = exploration_max\n",
        "        self.exploration_min = exploration_min\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "    # Store data to experience pool\n",
        "    def remember(self, state, action, reward, state2, done):\n",
        "        self.STATE_MEMORY[self.terminal_position] = state.float()\n",
        "        self.ACTION_MEMORY[self.terminal_position] = action.float()\n",
        "        self.REWARD_MEMORY[self.terminal_position] = reward.float()\n",
        "        self.STATE2_MEMORY[self.terminal_position] = state2.float()\n",
        "        self.DONE_MEMORY[self.terminal_position] = done.float()\n",
        "        self.terminal_position = (self.terminal_position + 1) % self.max_memory_size\n",
        "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
        "\n",
        "    def get_batch_experiences(self):\n",
        "        # Randomly batch size sample experiences\n",
        "        index = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
        "        STATE = self.STATE_MEMORY[index]\n",
        "        ACTION = self.ACTION_MEMORY[index]\n",
        "        REWARD = self.REWARD_MEMORY[index]\n",
        "        STATE2 = self.STATE2_MEMORY[index]\n",
        "        DONE = self.DONE_MEMORY[index]\n",
        "        return STATE, ACTION, REWARD, STATE2, DONE\n",
        "\n",
        "    def chosen_optimal_action(self, state):\n",
        "        optimal_action = None\n",
        "        if self.double_dqn:\n",
        "            # If it is DDQN, return the state input value local_net network to the action corresponding to the maximum network \n",
        "            # output value, and the network output position corresponds to the action position\n",
        "            optimal_action = torch.argmax(self.local_net(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
        "            return optimal_action\n",
        "        else:\n",
        "            # If it is DDQN, return the state input value dqn network to the action corresponding to the maximum output value \n",
        "            # of the network, and the network output position corresponds to the action position\n",
        "            optimal_action = torch.argmax(self.dqn(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
        "            return optimal_action\n",
        "\n",
        "    def chosen_action(self, state):\n",
        "        # Epsilon greedy policy\n",
        "        action_chosen = None\n",
        "        if self.double_dqn:\n",
        "            self.step += 1\n",
        "        # random exploration\n",
        "        if (np.random.rand() < self.exploration_rate):\n",
        "            # Randomly select an action from the action space and return\n",
        "            action_chosen = torch.tensor([[random.randrange(self.action_space)]])\n",
        "            return action_chosen\n",
        "        else:\n",
        "            action_chosen = self.chosen_optimal_action(state)\n",
        "            return action_chosen\n",
        "\n",
        "    def copy_model(self):\n",
        "        # Copy local_net weights to target_net every 5000 steps\n",
        "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
        "\n",
        "    def experience_replay(self):\n",
        "        if self.double_dqn and self.step % self.copy == 0:\n",
        "            # Copy local_net weights to target_net every 5000 steps\n",
        "            self.copy_model()\n",
        "\n",
        "        # If the experience pool is not full yet, return directly without training\n",
        "        if self.memory_sample_size > self.num_in_queue:\n",
        "            return\n",
        "        # After the experience pool is full, randomly obtain batch_size group data from the experience pool\n",
        "        STATE, ACTION, REWARD, STATE2, DONE = self.get_batch_experiences()\n",
        "\n",
        "        # Put the data on the GPU or cpu\n",
        "        STATE = STATE.to(self.device)\n",
        "        ACTION = ACTION.to(self.device)\n",
        "        REWARD = REWARD.to(self.device)\n",
        "        STATE2 = STATE2.to(self.device)\n",
        "        DONE = DONE.to(self.device)\n",
        "        # Optimizer gradient set to 0\n",
        "        self.optimizer.zero_grad()\n",
        "        if self.double_dqn:\n",
        "            #DDQN: Q*(S, A) <- r + gamma * max_a Q_target(S', a)  \n",
        "            target = REWARD + torch.mul((self.gamma * self.target_net(STATE2).max(1).values.unsqueeze(1)), 1 - DONE)\n",
        "            # Get the output current of the network\n",
        "            current = self.local_net(STATE).gather(1, ACTION.long())  # Local net approximation of Q-value\n",
        "        else:\n",
        "            # DQN: Q*(S, A) <- r + gamma * max_a Q(S', a)    \n",
        "            target = REWARD + torch.mul((self.gamma * self.dqn(STATE2).max(1).values.unsqueeze(1)), 1 - DONE)\n",
        "            # Get the output current of the network\n",
        "            current = self.dqn(STATE).gather(1, ACTION.long())\n",
        "        \n",
        "        # Huber loss as a loss function\n",
        "        loss = self.l1(current, target)\n",
        "        loss.backward()#gradients\n",
        "        self.optimizer.step()#Backpropagate\n",
        "        # Random exploration probability decay\n",
        "        self.exploration_rate = self.exploration_rate * self.exploration_decay\n",
        "        # 'exploration min' is always smaller or equal than epsilon\n",
        "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)\n",
        "\n",
        "def start(training_mode, pretrained, double_dqn, num_episodes=1000,actions=RIGHT_ONLY, exploration_max=1,save_path=\"./\",desc=\"\",render_mode=0):\n",
        "    '''\n",
        "        function: to train\n",
        "            parameter:\n",
        "                training_mode: True or False\n",
        "                    True: Indicates training mode\n",
        "                    False: non-training mode\n",
        "            pretrained: True or False\n",
        "                    True: Indicates that the trained model is loaded from save_path\n",
        "                    False: means not to load the trained model\n",
        "            double_dqn: True or False\n",
        "                    True: means use DDQN\n",
        "                    False: Indicates that DQN is used\n",
        "            num_episodes: int Set the number of games to loop\n",
        "            actions: RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT three action spaces\n",
        "            exploration_max: 0-1 The probability of initial random exploration, fixed at 1\n",
        "            save_path: folder path to save the model or load the model\n",
        "            desc: training display progress bar information\n",
        "            render_mode: The mode to display the test results, 0 means that the py file is tested locally, and 1 means that ipynb is tested in colab\n",
        "    '''\n",
        "    # Load 1-1 game environment object from gym_super_mario_bros library\n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "    # Process the obtained game scene\n",
        "    # Wraps the environment so that frames are grayscale\n",
        "    env = MSE(env)\n",
        "    env = MR84x84(env)\n",
        "    env = imgToTorch(env)\n",
        "    env = BW(env, 4)\n",
        "    env = PixelNormalization(env)\n",
        "    env = JoypadSpace(env, actions)\n",
        "\n",
        "    # Game obtains the input space size After the previous setting of the game environment, the game state is compressed into an array size of \n",
        "    # four consecutive frames: (4,84,84)\n",
        "    observation_space = env.observation_space.shape\n",
        "    # Game action space is the number of button combinations, len(RIGHT_ONLY)\n",
        "    action_space = env.action_space.n\n",
        "\n",
        "    agent = DQN_agent(state_space=observation_space,\n",
        "                     action_space=action_space,\n",
        "                     max_memory_size=30000, # Experience pool storage length\n",
        "                     batch_size=32, # Draw 32 from the experience pool each time as a batch\n",
        "                     gamma=0.90, # Indicates the importance of future rewards, the greater the value of future rewards\n",
        "                     lr=0.00025, # Network training learning rate\n",
        "                     dropout=0.2,  # Prevent overfitting The network does not use this parameter\n",
        "                     exploration_max=1.0, # Random action to explore the game space, the initial value is 1\n",
        "                     exploration_min=0.02, # Random action to explore the game space The final value after decay, indicating that there is a probability of 0.02 random action at the end\n",
        "                     exploration_decay=0.99, # Exploring Probability Decay Factors\n",
        "                     double_dqn=double_dqn,\n",
        "                     pretrained=pretrained,\n",
        "                     save_path=save_path)\n",
        "\n",
        "    # Reset the game for each episode\n",
        "    num_episodes = num_episodes # How many games to train\n",
        "    env.reset()\n",
        "\n",
        "    rewards_total = [] # Store rewards earned for each game\n",
        "    average_rewards_total={} # store the average reward\n",
        "    if training_mode and pretrained: # If you continue training, the previously saved rewards_total average_rewards_total will be loaded\n",
        "        with open(os.path.join(save_path,\"rewards_total.pkl\"), 'rb') as f:\n",
        "            rewards_total = pickle.load(f)\n",
        "        with open(os.path.join(save_path,\"average_rewards_total.pkl\"), 'rb') as f:\n",
        "            average_rewards_total = pickle.load(f)\n",
        "\n",
        "    for ep_num in tqdm(range(num_episodes),desc=desc): # Circuit training Show training progress bar\n",
        "        state = env.reset()   # Game environment initialization Return to the scene when the game is initialized, it is a numpy matrix of 4*84*84\n",
        "        state = torch.Tensor([state])\n",
        "        reward_total = 0    # The total reward is because the reward of a certain step returned by the gym every time is accumulated as the total reward\n",
        "        steps = 0    # The number of acts performed\n",
        "        while True:\n",
        "            if not training_mode: # If it's not training, show the Mario game screen\n",
        "                if render_mode==0:\n",
        "                    env.render()\n",
        "                elif render_mode==1:\n",
        "                    plt.figure(3)\n",
        "                    plt.clf()\n",
        "                    plt.imshow(env.render(mode='rgb_array'))\n",
        "                    plt.title(\"Episode: %d\" % (ep_num))\n",
        "                    plt.axis('off')\n",
        "                    display.clear_output(wait=True)\n",
        "                    display.display(plt.gcf())\n",
        "\n",
        "            action = agent.chosen_action(state)  # Get the actions that should be performed in the scene from the DQN_agent\n",
        "            steps = steps + 1    # Increase the number of actions by 1\n",
        "            next_state, reward, terminal, info = env.step(int(action[0])) # Get the updated state of the game after executing this act from the gym environment\n",
        "            reward_total = reward_total + reward\n",
        "            next_state = torch.Tensor([next_state])\n",
        "            reward = torch.tensor([reward]).unsqueeze(0)\n",
        "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
        "            if training_mode:\n",
        "                agent.remember(state, action, reward, next_state, terminal) # Put this result into the experience pool\n",
        "                agent.experience_replay() # Experience Playback Training\n",
        "            state = next_state\n",
        "            if terminal: # Game over\n",
        "                break\n",
        "        rewards_total.append(reward_total)\n",
        "\n",
        "        if ep_num % 100 == 0 and ep_num != 0: # Save the model and draw every n games\n",
        "            print(\"Episode {} score = {}, average score = {}\".format(ep_num + 1, rewards_total[-1],\n",
        "                                                                     np.mean(rewards_total)))\n",
        "\n",
        "            if training_mode:\n",
        "                # Drawing\n",
        "                average_rewards_total[ep_num] = np.mean(rewards_total)\n",
        "                plt.title(\"episodes & average reward (%s)\" % (\"DDQN\" if double_dqn else \"DQN\"))\n",
        "                plt.plot(list(average_rewards_total.keys()), list(average_rewards_total.values()))\n",
        "                plt.savefig(os.path.join(save_path, \"average_reward_plot.jpg\"))\n",
        "                plt.clf()\n",
        "                with open(os.path.join(save_path,\"terminal_position.pkl\"), \"wb\") as f:\n",
        "                    pickle.dump(agent.terminal_position, f)\n",
        "                with open(os.path.join(save_path,\"num_in_queue.pkl\"), \"wb\") as f:\n",
        "                    pickle.dump(agent.num_in_queue, f)\n",
        "                with open(os.path.join(save_path,\"rewards_total.pkl\"), \"wb\") as f:\n",
        "                    pickle.dump(rewards_total, f)\n",
        "                with open(os.path.join(save_path, \"average_rewards_total.pkl\"), \"wb\") as f:\n",
        "                    pickle.dump(rewards_total, f)\n",
        "\n",
        "                # Save network parameters\n",
        "                if agent.double_dqn:\n",
        "                    torch.save(agent.local_net.state_dict(), os.path.join(save_path,\"DQN1.pt\"))\n",
        "                    torch.save(agent.target_net.state_dict(), os.path.join(save_path,\"DQN2.pt\"))\n",
        "                else:\n",
        "                    torch.save(agent.dqn.state_dict(), os.path.join(save_path,\"DQN.pt\"))\n",
        "                torch.save(agent.ACTION_MEMORY, os.path.join(save_path,\"ACTION_MEMORY.pt\"))\n",
        "                torch.save(agent.REWARD_MEMORY, os.path.join(save_path,\"REWARD_MEMORY.pt\"))\n",
        "                torch.save(agent.DONE_MEMORY, os.path.join(save_path,\"DONE_MEMORY.pt\"))\n",
        "\n",
        "        num_episodes += 1\n",
        "\n",
        "    print(\"Episode {} score = {}, average score = {}\".format(ep_num + 1, rewards_total[-1], np.mean(rewards_total)))\n",
        "\n",
        "    # Save the trained memory so that we can continue from where we stop using 'pretrained' = True\n",
        "    if training_mode:\n",
        "        average_rewards_total[ep_num+1] = np.mean(rewards_total)\n",
        "        plt.title(\"episodes & average reward (%s)\"%(\"DDQN\" if double_dqn else \"DQN\"))\n",
        "        plt.plot(list(average_rewards_total.keys()), list(average_rewards_total.values()))\n",
        "        plt.savefig(os.path.join(save_path, \"average_reward_plot.jpg\"))\n",
        "        plt.clf()\n",
        "        with open(os.path.join(save_path, \"terminal_position.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(agent.terminal_position, f)\n",
        "        with open(os.path.join(save_path, \"num_in_queue.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(agent.num_in_queue, f)\n",
        "        with open(os.path.join(save_path, \"rewards_total.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(rewards_total, f)\n",
        "        with open(os.path.join(save_path, \"average_rewards_total.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(rewards_total, f)\n",
        "        # Save network parameters\n",
        "        if agent.double_dqn:\n",
        "            torch.save(agent.local_net.state_dict(), os.path.join(save_path, \"DQN1.pt\"))\n",
        "            torch.save(agent.target_net.state_dict(), os.path.join(save_path, \"DQN2.pt\"))\n",
        "        else:\n",
        "            torch.save(agent.dqn.state_dict(), os.path.join(save_path, \"DQN.pt\"))\n",
        "        torch.save(agent.ACTION_MEMORY, os.path.join(save_path, \"ACTION_MEMORY.pt\"))\n",
        "        torch.save(agent.REWARD_MEMORY, os.path.join(save_path, \"REWARD_MEMORY.pt\"))\n",
        "        torch.save(agent.DONE_MEMORY, os.path.join(save_path, \"DONE_MEMORY.pt\"))\n",
        "    env.close()\n",
        "    # Return the average reward situation, used for DDQN and DQN cooperative drawing comparison\n",
        "    return average_rewards_total\n",
        "\n",
        "\n",
        "def create_save_path(folder):\n",
        "    '''\n",
        "      Role: create a storage folder\n",
        "      introduce:\n",
        "          If the folder is a multi-level folder that does not exist, it will be created recursively at one time\n",
        "          And create a run folder in the folder directory. If the run folder exists, the run1 folder will be created, and if the run1 folder exists, the run2 folder will be created....\n",
        "     '''\n",
        "     \n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "    p1=os.path.join(folder, \"run\")\n",
        "    if  not os.path.exists(p1):\n",
        "        os.mkdir(p1)\n",
        "        return p1\n",
        "    else:\n",
        "        n=1\n",
        "        p1=os.path.join(folder,\"run%d\"%n)\n",
        "        while os.path.exists(p1):\n",
        "            n+=1\n",
        "            p1=os.path.join(folder,\"run%d\"%n)\n",
        "        os.mkdir(p1)\n",
        "        return p1\n",
        "\n",
        "\n",
        "def train_actions(to_actions=None,save_folder=\"checkpoints\", num_episodes=10000):\n",
        "    # actions={\"RIGHT_ONLY\":RIGHT_ONLY}\n",
        "    if to_actions is None:\n",
        "        to_actions = {\"RIGHT_ONLY\":RIGHT_ONLY}\n",
        "    run_path=create_save_path(save_folder)\n",
        "    for actions_name,actions in to_actions.items():\n",
        "        dqn_save_path=os.path.join(run_path,actions_name+\"_dqn\")\n",
        "        ddqn_save_path=os.path.join(run_path,actions_name+\"_ddqn\")\n",
        "        os.mkdir(dqn_save_path)\n",
        "        os.mkdir(ddqn_save_path)\n",
        "        dqn_average_rewards_total=start(training_mode=True, pretrained=False, double_dqn=False, num_episodes=num_episodes, actions=actions,exploration_max=1,save_path=dqn_save_path,desc=actions_name+\" dqn\")\n",
        "        ddqn_average_rewards_total=start(training_mode=True, pretrained=False, double_dqn=True, num_episodes=num_episodes, actions=actions,exploration_max=1,save_path=ddqn_save_path,desc=actions_name+\" ddqn\")\n",
        "        plt.title(\"episodes & average reward(%s)\"%actions_name)\n",
        "        plt.plot(list(ddqn_average_rewards_total.keys()), list(ddqn_average_rewards_total.values()), list(dqn_average_rewards_total.keys()), list(dqn_average_rewards_total.values()), \"y-\")\n",
        "        plt.legend([\"DDQN\", \"DQN\"], loc='best')\n",
        "        plt.savefig(os.path.join(run_path,\"DDQNvsDQN\"))\n",
        "        plt.clf()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOWQEgqXoYn6",
        "outputId": "f39585cc-2171-43af-d5a0-99aed9c4b9c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SIMPLE_MOVEMENT dqn Episode 101 score = 252.0, average score = 252.0\n",
            "SIMPLE_MOVEMENT dqn Episode 201 score = 1042.0, average score = 1042.0\n",
            "SIMPLE_MOVEMENT dqn Episode 301 score = 252.0, average score = 252.0\n",
            "SIMPLE_MOVEMENT dqn Episode 401 score = 636.0, average score = 636.0\n",
            "SIMPLE_MOVEMENT dqn Episode 501 score = 234.0, average score = 234.0\n",
            "SIMPLE_MOVEMENT dqn Episode 601 score = 1193.0, average score = 1193.0\n",
            "SIMPLE_MOVEMENT dqn Episode 701 score = 1285.0, average score = 1285.0\n",
            "SIMPLE_MOVEMENT dqn Episode 801 score = 607.0, average score = 607.0\n",
            "SIMPLE_MOVEMENT dqn Episode 901 score = 248.0, average score = 248.0\n",
            "SIMPLE_MOVEMENT dqn Episode 1001 score = 1325.0, average score = 1325.0\n",
            "SIMPLE_MOVEMENT dqn Episode 1101 score = 805.0, average score = 805.0\n",
            "SIMPLE_MOVEMENT dqn Episode 1201 score = 238.0, average score = 238.0\n",
            "SIMPLE_MOVEMENT dqn Episode 1301 score = 235.0, average score = 235.0\n",
            "SIMPLE_MOVEMENT dqn Episode 1401 score = 687.0, average score = 687.0\n",
            "SIMPLE_MOVEMENT dqn Episode 1501 score = 252.0, average score = 252.0\n",
            "SIMPLE_MOVEMENT dqn Episode 1601 score = 246.0, average score = 246.0\n",
            "SIMPLE_MOVEMENT dqn Episode 1701 score = 244.0, average score = 244.0\n",
            "SIMPLE_MOVEMENT dqn Episode 1801 score = 248.0, average score = 248.0\n",
            "SIMPLE_MOVEMENT dqn Episode 1901 score = 1377.0, average score = 1377.0\n",
            "SIMPLE_MOVEMENT dqn Episode 2001 score = 251.0, average score = 251.0\n",
            "SIMPLE_MOVEMENT dqn Episode 2101 score = 252.0, average score = 252.0\n",
            "SIMPLE_MOVEMENT dqn Episode 2201 score = 244.0, average score = 244.0\n",
            "SIMPLE_MOVEMENT dqn Episode 2301 score = 1701.0, average score = 1701.0\n",
            "SIMPLE_MOVEMENT dqn Episode 2401 score = 248.0, average score = 248.0\n",
            "SIMPLE_MOVEMENT dqn Episode 2501 score = 630.0, average score = 630.0\n",
            "SIMPLE_MOVEMENT dqn Episode 2601 score = 1309.0, average score = 1309.0\n",
            "SIMPLE_MOVEMENT dqn Episode 2701 score = 245.0, average score = 245.0\n",
            "SIMPLE_MOVEMENT dqn Episode 2801 score = 250.0, average score = 250.0\n",
            "SIMPLE_MOVEMENT dqn Episode 2901 score = 1586.0, average score = 1586.0\n",
            "SIMPLE_MOVEMENT dqn Episode 3001 score = 252.0, average score = 252.0\n",
            "SIMPLE_MOVEMENT dqn Episode 3101 score = 613.0, average score = 613.0\n",
            "SIMPLE_MOVEMENT dqn Episode 3201 score = 1324.0, average score = 1324.0\n",
            "SIMPLE_MOVEMENT dqn Episode 3301 score = 611.0, average score = 611.0\n",
            "SIMPLE_MOVEMENT dqn Episode 3401 score = 637.0, average score = 637.0\n",
            "SIMPLE_MOVEMENT dqn Episode 3501 score = 632.0, average score = 632.0\n",
            "SIMPLE_MOVEMENT dqn Episode 3601 score = 1013.0, average score = 1013.0\n",
            "SIMPLE_MOVEMENT dqn Episode 3701 score = 601.0, average score = 601.0\n",
            "SIMPLE_MOVEMENT dqn Episode 3801 score = 611.0, average score = 611.0\n",
            "SIMPLE_MOVEMENT dqn Episode 3901 score = 1440.0, average score = 1440.0\n",
            "SIMPLE_MOVEMENT dqn Episode 4001 score = 639.0, average score = 639.0\n",
            "SIMPLE_MOVEMENT dqn Episode 4101 score = 611.0, average score = 611.0\n",
            "SIMPLE_MOVEMENT dqn Episode 4201 score = 616.0, average score = 616.0\n",
            "SIMPLE_MOVEMENT dqn Episode 4301 score = 640.0, average score = 640.0\n",
            "SIMPLE_MOVEMENT dqn Episode 4401 score = 605.0, average score = 605.0\n",
            "SIMPLE_MOVEMENT dqn Episode 4501 score = 603.0, average score = 603.0\n",
            "SIMPLE_MOVEMENT dqn Episode 4601 score = 1344.0, average score = 1344.0\n",
            "SIMPLE_MOVEMENT dqn Episode 4701 score = 246.0, average score = 246.0\n",
            "SIMPLE_MOVEMENT dqn Episode 4801 score = 1339.0, average score = 1339.0\n",
            "SIMPLE_MOVEMENT dqn Episode 4901 score = 1149.0, average score = 1149.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 101 score = 250.0, average score = 250.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 201 score = 1321.0, average score = 1321.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 301 score = 611.0, average score = 611.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 401 score = 247.0, average score = 247.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 501 score = 638.0, average score = 638.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 601 score = 607.0, average score = 607.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 701 score = 242.0, average score = 242.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 801 score = 1322.0, average score = 1322.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 901 score = 608.0, average score = 608.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 1001 score = 610.0, average score = 610.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 1101 score = 1922.0, average score = 1922.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 1201 score = 237.0, average score = 237.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 1301 score = 610.0, average score = 610.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 1401 score = 809.0, average score = 809.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 1501 score = 612.0, average score = 612.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 1601 score = 610.0, average score = 610.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 1701 score = 1434.0, average score = 1434.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 1801 score = 252.0, average score = 252.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 1901 score = 1353.0, average score = 1353.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 2001 score = 624.0, average score = 624.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 2101 score = 1046.0, average score = 1046.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 2201 score = 608.0, average score = 608.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 2301 score = 1351.0, average score = 1351.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 2401 score = 1054.0, average score = 1054.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 2501 score = 628.0, average score = 628.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 2601 score = 1329.0, average score = 1329.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 2701 score = 611.0, average score = 611.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 2801 score = 1327.0, average score = 1327.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 2901 score = 2370.0, average score = 2370.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 3001 score = 1347.0, average score = 1347.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 3101 score = 654.0, average score = 654.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 3201 score = 1694.0, average score = 1694.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 3301 score = 1700.0, average score = 1700.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 3401 score = 250.0, average score = 250.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 3501 score = 2368.0, average score = 2368.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 3601 score = 2342.0, average score = 2342.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 3701 score = 611.0, average score = 611.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 3801 score = 1931.0, average score = 1931.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 3901 score = 2633.0, average score = 2633.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 4001 score = 2367.0, average score = 2367.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 4101 score = 1065.0, average score = 1065.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 4201 score = 3026.0, average score = 3026.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 4301 score = 1443.0, average score = 1443.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 4401 score = 2342.0, average score = 2342.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 4501 score = 3054.0, average score = 3054.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 4601 score = 3038.0, average score = 3038.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 4701 score = 1346.0, average score = 1346.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 4801 score = 249.0, average score = 249.0\n",
            "SIMPLE_MOVEMENT ddqn Episode 4901 score = 2654.0, average score = 2654.0\n",
            "<Figure size 432x288 with 0 Axes>\n"
          ]
        }
      ],
      "source": [
        "######train#####\n",
        "#actions is a list of action types that need to be trained. If you train others, add \"COMPLEX_MOVEMENT\":COMPLEX_MOVEMENT and \"RIGHT_ONLY\":RIGHT_ONLY, and each type of dqn and ddqn in the list will be trained in turn\n",
        "#save_fodel is the folder where the training results are saved. If it does not exist, it will be recursively created. Each training will generate a run folder, and multiple times will generate run1, run2, run3...\n",
        "\n",
        "actions={\"SIMPLE_MOVEMENT\":SIMPLE_MOVEMENT}\n",
        "save_fodel=\"drive/MyDrive/DQN_DDQN/SIMPLE_MOVEMENT_checkpoints\"\n",
        "train_actions(to_actions=actions,save_folder=save_fodel,num_episodes=5000)\n",
        "\n",
        "# We lost the original output results due to mistaken touch, and finally re-imported the previous output results using pickle and os packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjxSKAb42S_u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "CsxyjrexoeLW",
        "outputId": "af97d227-46e3-47cc-8d92-f804c7182391"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [01:00<00:00, 60.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1 score = 678.0, average score = 678.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "######test######\n",
        "# save_root The run folder path after training\n",
        "# The models in the actions folder are suitable for that type of action\n",
        "# satrt parameter\n",
        "#       training_mode=False, pretrained=True means testing\n",
        "#       double_dqn: True if the model in the folder is DDQN and False if DQN\n",
        "#       num_episodes: set the number of tests, do not set too large in colab, it is easy to collapse the memory\n",
        "#       render_mode: display mode, if it is a py file for local testing, it is 0, and ipynb is 1\n",
        "#       exploration_max: The probability of performing random actions can be set to a very small number during the test\n",
        "#######Example####\n",
        "save_root=\"drive/MyDrive/DQN_DDQN/SIMPLE_MOVEMENT_checkpoints/run1/SIMPLE_MOVEMENT_ddqn\"\n",
        "actions=SIMPLE_MOVEMENT\n",
        "start(training_mode=False, pretrained=True, double_dqn=True, num_episodes=1, render_mode=1,actions=actions,exploration_max=0.05, save_path=save_root, desc=\"\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}